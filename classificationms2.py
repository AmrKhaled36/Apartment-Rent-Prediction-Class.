# -*- coding: utf-8 -*-
"""ClassificationMS2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/163e-LSiHSdEk93--MilJBEwxSgBHzhcx
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import f_classif
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier
from mlxtend.classifier import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import pickle
import time

df = pd.read_csv('ApartmentRentPrediction_Milestone2.csv')

df.describe()

df.info()

"""# Preprocessing"""

df.dropna(subset=['longitude','latitude', 'state','cityname','bedrooms','bathrooms'], inplace=True)

pets_allowed_mvalue = df['pets_allowed'].mode()[0]

df['pets_allowed'].fillna(pets_allowed_mvalue, inplace=True)

df['address'] = df.apply(lambda row: row['cityname'] if pd.isnull(row['address']) else row['address'], axis=1)
####################################


df.isnull().sum()

len(df['address'].unique())

for i, row in df.iterrows():
  if row['RentCategory'] == 'Low Rent':
    df.at[i, 'RentCategory'] = 1

  elif row['RentCategory'] == 'Medium-Priced Rent':
    df.at[i, 'RentCategory'] = 2

  elif row['RentCategory'] == 'High Rent':
    df.at[i, 'RentCategory'] = 3

  else:
    df.at[i, 'RentCategory'] = 4

df

# Input string containing substrings separated by commas
input_string = df['amenities']
substring_set = dict()
for value in input_string:
  if type(value) != str:
    continue
  substrings = value.split(',')
  for j in substrings:
    substring_set[j] = [0, 0]

substring_set

for i in range(df.shape[0]):
  # print(df.iloc[i, 4], i)
  if type(df.iloc[i, 4]) != str:
    continue
  s = df.iloc[i, 4].split(',')
  for j in s:
    substring_set[j][0] += df.iloc[i, 11]
    substring_set[j][1] += 1

l = []

for key, value in substring_set.items():
  l.append((value[0]/value[1], key))

l.sort()
for i in range(len(l)):
  substring_set[l[i][1]] = i

for i in range(df.shape[0]):
  if type(df.iloc[i, 4]) != str:
    df.iloc[i, 4] = 0
    continue
  s = df.iloc[i, 4].split(',')
  tmp = 0
  for j in s:
    tmp += (1 << (substring_set[j]))
  df.iloc[i, 4] = tmp

df.head(20)

# Apply ceil function

df['bathrooms'] = df['bathrooms'].apply(np.ceil)
df['bedrooms'] = df['bedrooms'].apply(np.ceil)

# Identify numeric columns
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Detect columns with outliers
columns_with_outliers = []
for column in numeric_columns:
    z_scores = stats.zscore(df[column], nan_policy = 'omit')
    if (z_scores < -3).any():
      columns_with_outliers.append(column)
    elif (z_scores > 3).any():
      columns_with_outliers.append(column)

if columns_with_outliers:
    print("Columns with outliers:", columns_with_outliers)
else:
    print("No columns with outliers detected.")

plt.figure(figsize=(8, 6))
plt.boxplot(df['bathrooms'])
plt.title('Box Plot of bathrooms')
plt.ylabel('Bathrooms')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

plt.figure(figsize=(8, 6))
plt.boxplot(df['bedrooms'])
plt.title('Box Plot of bedrooms')
plt.ylabel('Bedrooms')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

plt.figure(figsize=(8, 6))
plt.boxplot(df['square_feet'])
plt.title('Box Plot of square_feet')
plt.ylabel('Square_feet')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

plt.figure(figsize=(8, 6))
plt.boxplot(df['latitude'])
plt.title('Box Plot of latitude')
plt.ylabel('Latitude')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

plt.figure(figsize=(8, 6))
plt.boxplot(df['longitude'])
plt.title('Box Plot of longitude')
plt.ylabel('Longitude')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

# Calculate z-scores for each value in the DataFrame
Z = stats.zscore(df.select_dtypes(include=['int64', 'float64']), nan_policy = 'omit')

#Drop outliers with z-score equal to or greater than 3
df.drop(df.index[(np.abs(Z) > 3).any(axis=1)], inplace=True)

print(df.isnull().sum())

plt.figure(figsize=(8, 6))  # Set the size of the plot
plt.boxplot(df['bathrooms'])
plt.title('Box Plot of bathrooms')
plt.ylabel('bathrooms')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

plt.figure(figsize=(8, 6))
plt.boxplot(df['bedrooms'])
plt.title('Box Plot of bedrooms')
plt.ylabel('Bedrooms')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

plt.figure(figsize=(8, 6))
plt.boxplot(df['latitude'])
plt.title('Box Plot of latitude')
plt.ylabel('Latitude')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

plt.figure(figsize=(8, 6))
plt.boxplot(df['longitude'])
plt.title('Box Plot of longitude')
plt.ylabel('Longitude')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

plt.figure(figsize=(8, 6))
plt.boxplot(df['square_feet'])
plt.title('Box Plot of square_feet')
plt.ylabel('Square_feet')
plt.grid(True)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

sns.displot(df['square_feet'])

# Encode categorical columns
cat_cols = ['category', 'title', 'body', 'currency', 'fee','has_photo', 'pets_allowed', 'price_type', 'source']
def Feature_Encoder(df,cat_cols):
    for c in cat_cols:
        lbl = LabelEncoder()
        lbl.fit(list(df[c].values))
        df[c] = lbl.transform(list(df[c].values))
    return df

def fet_encod(x, y):
    average_values = x.groupby(y)['RentCategory'].mean()
    sorted_average_values = average_values.sort_values()
    x[y] = x[y].map(sorted_average_values.rank())
    print(sorted_average_values.rank())
    with open(f'{y}_class.pkl', 'wb') as model_file:
      pickle.dump(sorted_average_values, model_file)
fet_encod(df,"cityname")
fet_encod(df,'state')
fet_encod(df,"address")

cityname_mode = df['cityname'].mode()[0]
state_mode = df['state'].mode()[0]
bathrooms_mode = df['bathrooms'].mode()[0]
square_feet_mean = df['square_feet'].mean()

with open('fillna_class.pkl', 'wb') as model_file:
    pickle.dump({'cityname': cityname_mode, 'state':state_mode, 'bathrooms': bathrooms_mode, 'square_feet': square_feet_mean}, model_file)

# Min-Max Scaling on address
df['address'] = (df['address'] - df['address'].min()) / (df['address'].max() - df['address'].min())

#Encoding ID column
tmp = df.copy()
df['id'] = range(1, len(df) + 1)
Feature_Encoder(df,cat_cols)
df.head()

"""# Visualizing"""

price_type_counts = tmp['price_type'].value_counts()

# Create the bar chart
plt.figure(figsize=(8, 6))
price_type_counts.plot(kind='bar', color='skyblue')
plt.title('Distribution of Price Types')
plt.xlabel('Price Type')
plt.ylabel('Count')
plt.xticks(rotation=0)  # Rotate x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines to the y-axis
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

# Count the occurrences of 'Yes' and 'No' in the 'pets_allowed' column
pets_allowed_counts = tmp['pets_allowed'].value_counts()
# Create the pie chart
plt.figure(figsize=(8, 6))
plt.pie(pets_allowed_counts, labels=pets_allowed_counts.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightgreen', 'red'])
plt.title('Pets Allowed Distribution')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

# Count the occurrences of 'Yes' and 'No' in the 'pets_allowed' column
RentCategory_counts = tmp['RentCategory'].value_counts()
# Create the pie chart
plt.figure(figsize=(8, 6))
plt.pie(RentCategory_counts, labels=RentCategory_counts.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightgreen', 'red'])
plt.title('RentCategory Distribution')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

X = df[['cityname', 'address','state', 'pets_allowed', 'has_photo', 'fee', 'currency', 'source', 'price_type', 'title', 'category']]  # Features
y = df['RentCategory'].astype(str)  # Target variable

# Calculate mutual information scores for classification
mi_scores = mutual_info_classif(X, y)

# Create a DataFrame for easier plotting
mi_df = pd.DataFrame({'Features': X.columns, 'Mutual_Information_Score': mi_scores})

# Plotting a heatmap of mutual information scores
plt.figure(figsize=(8, 6))
sns.heatmap(mi_df.pivot_table(index='Features', columns=None, values='Mutual_Information_Score'), annot=True, cmap='YlGnBu')
plt.title('Mutual Information Scores Heatmap')
plt.show()

X = df[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'time', 'amenities']]  # Features
y = df['RentCategory']  # Target variable

# Calculate ANOVA F-values for classification
f_values, p_values = f_classif(X, y)

# Create a DataFrame for easier plotting
anova_df = pd.DataFrame({'Features': X.columns, 'ANOVA_F_Value': f_values})

# Plotting a bar plot of ANOVA F-values
plt.figure(figsize=(8, 6))
sns.barplot(x='ANOVA_F_Value', y='Features', data=anova_df, palette='viridis')
plt.xlabel('ANOVA F-Value')
plt.ylabel('Features')
plt.title('ANOVA F-Values for Features')
plt.show()

df.drop(columns=['address','category', 'title', 'body', 'currency', 'fee','has_photo', 'pets_allowed', 'price_type', 'source', 'latitude', 'longitude', 'id', 'time', 'amenities'], inplace = True)

df.head()

"""# **Models**

**SVM**
"""

X = df[['square_feet', 'bathrooms', 'cityname', 'state']]  # Features ###############
y = df['RentCategory'].astype(str)  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)

# Initialize SVM classifier
svm_classifier = SVC(kernel='rbf', gamma=10, probability=True, random_state=42)

# Train the classifier
train_time_start_SVM = time.time()
svm_classifier.fit(X_train, y_train)
train_time_end_SVM = time.time()

train_time_SVM = train_time_end_SVM - train_time_start_SVM

# Make predictions on the test set
test_time_start_SVM = time.time()
y_pred = svm_classifier.predict(X_test)
test_time_end_SVM = time.time()

test_time_SVM = test_time_end_SVM - test_time_start_SVM

y_train_pred = svm_classifier.predict(X_train)

# Evaluate the classifier
accuracySVM = accuracy_score(y_test, y_pred)
accuracy2 = accuracy_score(y_train, y_train_pred)

report = classification_report(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

print("Accuracy(train):", accuracy2)
print("Accuracy(test):", accuracySVM)
print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion_mat)

with open('SVM_class.pkl', 'wb') as model_file:
    pickle.dump(svm_classifier, model_file)

"""**Decision Tree**"""

# Initialize decision tree classifier
dt_classifier = DecisionTreeClassifier(
    max_depth=8,
    random_state=42,
    )

# Train the classifier
train_time_start_DT = time.time()
dt_classifier.fit(X_train, y_train)
train_time_end_DT = time.time()

train_time_DT = train_time_end_DT - train_time_start_DT
# Make predictions on the test set
test_time_start_DT = time.time()
y_pred = dt_classifier.predict(X_test)
test_time_end_DT = time.time()

test_time_DT = test_time_end_DT - test_time_start_DT
y_train_pred = dt_classifier.predict(X_train)

# Evaluate the classifier
accuracyDT = accuracy_score(y_test, y_pred)
accuracy2 = accuracy_score(y_train, y_train_pred)

report = classification_report(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

print("Accuracy(train):", accuracy2)
print("Accuracy(test):", accuracyDT)
print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion_mat)

"""**Naive Bayes**"""

nb_classifier = GaussianNB()

train_time_start_NB = time.time()
nb_classifier.fit(X_train, y_train)
train_time_end_NB = time.time()

train_time_NB = train_time_end_NB - train_time_start_NB
# Make predictions on the test set
test_time_start_NB = time.time()
y_pred = nb_classifier.predict(X_test)
test_time_end_NB = time.time()

test_time_NB = test_time_end_NB - test_time_start_NB

y_train_pred = nb_classifier.predict(X_train)

# Evaluate the classifier
accuracyNB = accuracy_score(y_test, y_pred)
accuracy2 = accuracy_score(y_train, y_train_pred)
report = classification_report(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

print("Accuracy(train):", accuracy2)
print("Accuracy(test):", accuracyNB)
print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion_mat)

"""**KNN**"""

knn_classifier = KNeighborsClassifier(n_neighbors=30)

train_time_start_KNN = time.time()
knn_classifier.fit(X_train, y_train)
train_time_end_KNN = time.time()

train_time_KNN = train_time_end_KNN - train_time_start_KNN
# Make predictions on the test set
test_time_start_KNN = time.time()
y_pred = knn_classifier.predict(X_test)
test_time_end_KNN = time.time()

test_time_KNN = test_time_end_KNN - test_time_start_KNN

y_train_pred = nb_classifier.predict(X_train)

# Evaluate the classifier
accuracyKNN = accuracy_score(y_test, y_pred)
accuracy2 = accuracy_score(y_train, y_train_pred)
report = classification_report(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

print("Accuracy(train):", accuracy2)
print("Accuracy(test):", accuracyKNN)
print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion_mat)

"""**Ensemble Learning(Voting)**"""

voting_classifier = VotingClassifier(estimators=[('dt', dt_classifier), ('svm', svm_classifier), ('knn', knn_classifier),('nb', nb_classifier)], voting='hard')

# Train the classifier
train_time_start_ELV = time.time()
voting_classifier.fit(X_train, y_train)
train_time_end_ELV = time.time()

train_time_ELV = train_time_end_ELV - train_time_start_ELV
# Make predictions on the test set
test_time_start_ELV = time.time()
y_pred_voting = voting_classifier.predict(X_test)
test_time_end_ELV = time.time()

test_time_ELV = test_time_end_ELV - test_time_start_ELV

y_pred_voting_train = voting_classifier.predict(X_train)

# Evaluate the classifier
accuracy_votingELV = accuracy_score(y_test, y_pred_voting)
accuracy_voting2 = accuracy_score(y_train, y_pred_voting_train)
report_voting = classification_report(y_test, y_pred_voting)
confusion_mat_voting = confusion_matrix(y_test, y_pred_voting)

print("Voting Classifier Accuracy(train):", accuracy_voting2)
print("Voting Classifier Accuracy(test):", accuracy_votingELV)
print("Voting Classifier Classification Report:\n", report_voting)
print("Voting Classifier Confusion Matrix:\n", confusion_mat_voting)

"""**Ensemble Learning(Stacking)**"""

base_classifiers = [
    dt_classifier,
    svm_classifier,
    knn_classifier,
    nb_classifier
]

meta_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# Initialize stacking classifier
stacking_classifier = StackingClassifier(classifiers=base_classifiers,
                                         meta_classifier=meta_classifier,
                                         use_probas=True)

# Train the stacking classifier
train_time_start_ELS = time.time()
stacking_classifier.fit(X_train, y_train)
train_time_end_ELS = time.time()

train_time_ELS = train_time_end_ELS - train_time_start_ELS
# Make predictions on the test set
test_time_start_ELS = time.time()
y_pred_stacking = stacking_classifier.predict(X_test)
test_time_end_ELS = time.time()

test_time_ELS = test_time_end_ELS - test_time_start_ELS

y_pred_stacking_train = stacking_classifier.predict(X_train)
# Evaluate the classifier
accuracy_stackingELS = accuracy_score(y_test, y_pred_stacking)
accuracy_stacking2 = accuracy_score(y_train, y_pred_stacking_train)

print("Stacking Classifier Accuracy(train):", accuracy_stacking2)
print("Stacking Classifier Accuracy(test):", accuracy_stackingELS)

with open('ELSR_class.pkl', 'wb') as model_file:
    pickle.dump(stacking_classifier, model_file)

"""# Models Stats"""

plt.figure(figsize=(10, 6))
categories = ['SVM', 'Decision Tree', 'Naive Bayes', 'KNN', 'ELV', 'ELS']
values = [accuracySVM, accuracyDT, accuracyNB, accuracyKNN, accuracy_votingELV, accuracy_stackingELS]
plt.bar(categories, values)
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Accuracy Statistics')
plt.show()

plt.figure(figsize=(10, 6))
categories = ['SVM', 'Decision Tree', 'Naive Bayes', 'KNN', 'ELV', 'ELS']
values = [train_time_SVM, train_time_DT, train_time_NB, train_time_KNN, train_time_ELV, train_time_ELS]
plt.bar(categories, values)
plt.xlabel('Models')
plt.ylabel('Train Time')
plt.title('Time Statistics')
plt.show()

plt.figure(figsize=(10, 6))
categories = ['SVM', 'Decision Tree', 'Naive Bayes', 'KNN', 'ELV', 'ELS']
values = [test_time_SVM, test_time_DT, test_time_NB, test_time_KNN, test_time_ELV, test_time_ELS]
plt.bar(categories, values)
plt.xlabel('Models')
plt.ylabel('Test Time')
plt.title('Time Statistics')
plt.show()